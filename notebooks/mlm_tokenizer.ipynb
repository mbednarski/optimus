{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset\n",
    "import tokenizers.normalizers as tn\n",
    "import tokenizers.pre_tokenizers as tp\n",
    "import tokenizers.models as tm\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.decoders import WordPiece\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset wikitext (/home/xevaquor/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91)\n"
     ]
    }
   ],
   "source": [
    "wiki = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('wiki.raw', 'wt') as f:\n",
    "    for l in wiki['train']['text']:\n",
    "        f.write(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10_000\n",
    "\n",
    "model = tm.WordPiece()\n",
    "\n",
    "tokenizer = Tokenizer(model)\n",
    "tokenizer.normalizer = tn.Sequence([\n",
    "    tn.NFD(), # unicode normalization\n",
    "    tn.StripAccents(), # strip accents\n",
    "    tn.Lowercase(),\n",
    "    tn.Strip() # strip starting and trailing whitespaces\n",
    "])\n",
    "tokenizer.pre_tokenizer = tp.Whitespace() # pretokenize on spaces\n",
    "# add starting [CLS] token\n",
    "tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A\", special_tokens=[('[CLS]', 1)])\n",
    "\n",
    "tokenizer.decoder = WordPiece()\n",
    "\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[PAD]\", \"[MASK]\"]    \n",
    ")\n",
    "\n",
    "\n",
    "tokenizer.train(trainer, ['wiki.raw'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('tokenizer.json', pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer.from_file('tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "seq_len = 10\n",
    "tok.enable_padding(pad_token='[PAD]', length=seq_len)\n",
    "tok.enable_truncation(max_length=seq_len)\n",
    "enc = tok.encode('I like pizza')\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'like', 'p', '##iz', '##za', '[PAD]', '[PAD]', '[PAD]', '[PAD]']"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "enc.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "enc.special_tokens_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "enc.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 50, 2026, 57, 3, 9861, 0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "ids = enc.ids\n",
    "ids[4] = tok.token_to_id('[MASK]')\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'i like pza'"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "tok.decode(ids)"
   ]
  }
 ]
}